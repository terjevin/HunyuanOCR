import os
import torch
import numpy as np
from PIL import Image
from transformers import AutoProcessor
from qwen_vl_utils import process_vision_info  # è¯·ç¡®ä¿è¯¥æ¨¡å—åœ¨ä½ çš„ç¯å¢ƒå¯ç”¨
from transformers import HunYuanVLV1ForConditionalGeneration

import json
import tqdm

# ======================
# å…¨å±€å˜é‡ï¼šæ¨¡å‹ä¸å¤„ç†å™¨ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
# ======================

# æ¨¡å‹å’Œprocessoråˆå§‹åŒ–ä¸º None
model = None
processor = None
model_loaded = False

MODEL_PATH = "/data/manayang/500m_vl_ocr_1112_format/"  # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹

def load_model_once():
    """åŠ è½½æ¨¡å‹å’Œprocessorï¼Œä»…éœ€æ‰§è¡Œä¸€æ¬¡"""
    global model, processor, model_loaded
    if model_loaded:
        return

    print("[INFO] æ­£åœ¨åŠ è½½ Hunyuan-VL æ¨¡å‹å’Œ Processorï¼ˆä»…éœ€ä¸€æ¬¡ï¼‰...")
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"

    # åŠ è½½ processor
    processor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=False, trust_remote_code=True)

    # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨ bfloat16 + cudaï¼Œæ³¨æ„è®¾ç½®å¥½ attn_implementation
    model = HunYuanVLV1ForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        attn_implementation="eager",  # ä¹Ÿå¯ä»¥æ˜¯ flash_attention_2 æˆ– sdpaï¼Œæ ¹æ®ä½ çš„ç¯å¢ƒæ”¯æŒæƒ…å†µé€‰æ‹©
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    model_loaded = True
    print("[INFO] æ¨¡å‹åŠ è½½å®Œæˆ âœ…ï¼Œå¯ä»¥å¼€å§‹æ¨ç†äº†ï¼")

# ======================
# æ¨ç†å‡½æ•°ï¼šè¾“å…¥ text å’Œ img_pathï¼Œè¿”å› output_texts
# ======================
def inference_hunyuan_vl(text: str, img_path: str) -> list:
    """
    ä½¿ç”¨å·²ç»åŠ è½½çš„ Hunyuan-VL æ¨¡å‹ï¼Œå¯¹è¾“å…¥çš„å›¾ç‰‡å’Œæ–‡æœ¬è¿›è¡Œæ¨ç†ã€‚

    Args:
        text (str): ç”¨æˆ·æé—®ï¼Œæ¯”å¦‚è¦æå–å›¾ç‰‡ä¸­çš„å“ªäº›å­—æ®µ
        img_path (str): å›¾ç‰‡çš„æœ¬åœ°è·¯å¾„

    Returns:
        list: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ç»“æœï¼Œé€šå¸¸æ˜¯ [output_str]
    """
    global model, processor

    if not model_loaded:
        raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆè°ƒç”¨ load_model_once() æˆ–ç¡®ä¿æ¨ç†å‰å·²åŠ è½½ã€‚")

    # æ„é€ å¯¹è¯è¾“å…¥
    messages1 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": img_path},
                {"type": "text", "text": text},
            ],
        }
    ]
    messages = [messages1]

    # ä½¿ç”¨ processor æ„é€ è¾“å…¥æ ¼å¼
    texts = [
        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
        for msg in messages
    ]
    image_inputs, video_inputs = process_vision_info(messages)

    inputs = processor(
        text=texts,
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to("cuda")

    # æ¨¡å‹æ¨ç†
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024*8,
            repetition_penalty=1.03,
            do_sample=False
        )

    # è§£ç è¾“å‡º
    if "input_ids" in inputs:
        input_ids = inputs.input_ids
    else:
        input_ids = inputs.inputs  # fallback

    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(input_ids, generated_ids)
    ]

    output_texts = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )

    return output_texts


# ======================
# ä½¿ç”¨ç¤ºä¾‹ ğŸ‘‡
# ======================
if __name__ == '__main__':
    # ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ¨¡å‹ï¼ˆåªä¼šæ‰§è¡Œä¸€æ¬¡ï¼ï¼‰
    load_model_once()

    with open("./ocr_test_data_response_bf16.jsonl",
              "r", encoding="utf-8") as f:
        with open("./ocr_test_data_response_bf16_1115.jsonl",
              "w", encoding="utf-8") as fout:
            for line in tqdm.tqdm(f):
                data = json.loads(line)
                question  = data["question"]
                img_path = data["img_path"]
                img_path = img_path.replace("./images/", "/apdcephfs_gy2/share_302508627/manayang/mm_sh/test_images/")
                result = inference_hunyuan_vl(text=question, img_path=img_path)
                data["response"] = result
                str_out = json.dumps(data, ensure_ascii=False) + "\n"
                print(f"{question} ||||| {result}")
                fout.write(str_out)

    # img_path = "/apdcephfs_gy2/share_302508627/manayang/mm_sh/test_images/0390fdf43974b617001ad98d6bea0ba7.jpg"
    # query = "æå–å›¾ç‰‡ä¸­çš„ï¼š['*å°¿ç´ çš„å•ä½', '*å°¿ç´ çš„å‚è€ƒèŒƒå›´', '*ç™½è›‹ç™½çš„ç¼©å†™', '*é—¨å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶çš„å‚è€ƒèŒƒå›´', 'ç™½/çƒè›‹ç™½æ¯”å€¼çš„ç»“æœ', 'è¡€æµ†æ¸—é€å‹(è®¡ç®—æ³•)çš„å‚è€ƒèŒƒå›´', '*æ— æœºç£·çš„å•ä½', '*æ°¯çš„ç¼©å†™', '*èƒ†å›ºé†‡çš„ç¼©å†™']çš„å­—æ®µå†…å®¹ï¼Œå¹¶ä¸”æŒ‰ç…§JSONæ ¼å¼è¿”å›ã€‚"

    # result = inference_hunyuan_vl(text=query, img_path=img_path)
    # print("æ¨ç†ç»“æœ:", result)
