<div align="center">

[English Version](./README.md)

</div>

<div align="center">

# æ··å…ƒOCRï¼šæ··å…ƒåŸç”Ÿå¤šæ¨¡æ€ç«¯åˆ°ç«¯ OCR ä¸“å®¶ï¼Œ1B è½»é‡åŒ–å‚æ•°å´æ–©è·å¤šé¡¹ä¸šç•Œ SOTAï¼

</div>
<p align="center">
 <img src="./assets/hyocr-head-img.png" width="80%"/> <br>
</p>


<p align="center">
<a href="https://huggingface.co/spaces/tencent/HunyuanOCR"><b>ğŸ¯ åœ¨çº¿Demoä½“éªŒ</b></a> |
<a href="https://huggingface.co/tencent/HunyuanOCR"><b>ğŸ“¥ æ¨¡å‹æƒé‡ä¸‹è½½</b></a> |
<a href="https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/HunyuanOCR_Technical_Report.pdf"><b>ğŸ“„ æŠ€æœ¯æŠ¥å‘Š</b></a>
</p>

## ğŸ”¥ æœ€æ–°åŠ¨æ€
- **[2025/11/25]** ğŸ“ æ¨ç†ä»£ç å’Œæ¨¡å‹æƒé‡å·²å¼€æºã€‚

## ğŸ“– ç®€ä»‹
**HunyuanOCR**æ˜¯ä¸€æ¬¾åŸºäºè…¾è®¯æ··å…ƒåŸç”Ÿå¤šæ¨¡æ€æ¶æ„çš„ç«¯åˆ°ç«¯OCRä¸“å®¶æ¨¡å‹ã€‚ä»…ä»¥**1B**è½»é‡åŒ–å‚æ•°ï¼Œä¾¿å·²æ–©è·å¤šé¡¹ä¸šç•ŒSOTAæˆç»©ã€‚è¯¥æ¨¡å‹ç²¾é€š**å¤æ‚å¤šè¯­ç§æ–‡æ¡£è§£æ**ï¼ŒåŒæ—¶åœ¨**æ–‡å­—æ£€æµ‹è¯†åˆ«ã€å¼€æ”¾å­—æ®µä¿¡æ¯æŠ½å–ã€è§†é¢‘å­—å¹•è¯†åˆ«ã€æ‹ç…§ç¿»è¯‘**ç­‰å…¨åœºæ™¯å®ç”¨æŠ€èƒ½ä¸­è¡¨ç°å‡ºè‰²ã€‚


## âœ¨ æ ¸å¿ƒç‰¹ç‚¹

- ğŸ’ª **è½»é‡åŒ–æ¶æ„**ï¼šåŸºäºæ··å…ƒåŸç”Ÿå¤šæ¨¡æ€æ¶æ„ä¸è®­ç»ƒç­–ç•¥ï¼Œæ‰“é€ ä»…1Bå‚æ•°çš„OCRä¸“é¡¹æ¨¡å‹ï¼Œå¤§å¹…é™ä½éƒ¨ç½²æˆæœ¬ã€‚

- ğŸ“‘ **å…¨åœºæ™¯åŠŸèƒ½**ï¼šå•ä¸€æ¨¡å‹è¦†ç›–æ–‡å­—æ£€æµ‹å’Œè¯†åˆ«ã€å¤æ‚æ–‡æ¡£è§£æã€å¡è¯ç¥¨æ®å­—æ®µæŠ½å–ã€å­—å¹•æå–ç­‰OCRç»å…¸ä»»åŠ¡ï¼Œæ›´æ”¯æŒç«¯åˆ°ç«¯æ‹ç…§ç¿»è¯‘ä¸æ–‡æ¡£é—®ç­”ã€‚

- ğŸš€ **æè‡´æ˜“ç”¨**ï¼šæ·±åº¦è´¯å½»å¤§æ¨¡å‹"ç«¯åˆ°ç«¯"ç†å¿µï¼Œå•ä¸€æŒ‡ä»¤ã€å•æ¬¡æ¨ç†ç›´è¾¾SOTAç»“æœï¼Œè¾ƒä¸šç•Œçº§è”æ–¹æ¡ˆæ›´é«˜æ•ˆä¾¿æ·ã€‚

- ğŸŒ **å¤šè¯­ç§æ”¯æŒ**ï¼šæ”¯æŒè¶…è¿‡100ç§è¯­è¨€ï¼Œåœ¨å•è¯­ç§å’Œæ··åˆè¯­è¨€åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ã€‚

<div align="left">
  <img src="./assets/hyocr-pipeline.png" alt="HunyuanOCRæ¡†æ¶" width="80%">
</div>

## ğŸ› ï¸ ç¯å¢ƒä¾èµ–ä¸å®‰è£…

### ç³»ç»Ÿè¦æ±‚
- ğŸ–¥ï¸ æ“ä½œç³»ç»Ÿï¼šLinux
- ğŸ Pythonç‰ˆæœ¬ï¼š3.12+ï¼ˆæ¨èï¼‰
- âš¡ CUDAç‰ˆæœ¬ï¼š12.8
- ğŸ”¥ PyTorchç‰ˆæœ¬ï¼š2.7.1
- ğŸ® GPUï¼šæ”¯æŒCUDAçš„NVIDIAæ˜¾å¡
- ğŸ§  GPUæ˜¾å­˜ï¼š80GB 
- ğŸ’¾ ç£ç›˜ç©ºé—´ï¼š6GB

## ğŸš€ åŸºäºvLLMå¿«é€Ÿä½¿ç”¨

### å®‰è£…æ­¥éª¤
```bash
pip install vllm --extra-index-url https://wheels.vllm.ai/nightly
```

### æ¨¡å‹æ¨ç†

```python
from vllm import LLM, SamplingParams
from PIL import Image
from transformers import AutoProcessor

model_path = "tencent/HunyuanOCR"
llm = LLM(model=model_path, trust_remote_code=True)
processor = AutoProcessor.from_pretrained(model_path)
sampling_params = SamplingParams(temperature=0, max_tokens=16384)

img_path = "/path/to/image.jpg"
img = Image.open(img_path)
messages = [
    {"role": "user", "content": [
        {"type": "image", "image": img_path},
        {"type": "text", "text": "Detect and recognize text in the image, and output the text coordinates in a formatted manner."}
    ]}
]
prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = {"prompt": prompt, "multi_modal_data": {"image": [img]}}
output = llm.generate([inputs], sampling_params)[0]
print(output.outputs[0].text)
```


## ğŸš€ åŸºäºTransformerså¿«é€Ÿä½¿ç”¨

### å®‰è£…æ­¥éª¤

```bash
pip install git+https://github.com/huggingface/transformers@82a06db03535c49aa987719ed0746a76093b1ec4
```

> **Note**: æˆ‘ä»¬ç¨åä¼šå°†å…¶åˆå¹¶åˆ°Transformersä¸»åˆ†æ”¯ä¸­ã€‚

### æ¨¡å‹æ¨ç†

HunyuanOCR æä¾›ç›´è§‚çš„æ¨¡å‹æ¨ç†æ¥å£ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨æŒ‡å¼•ï¼š

```python
from transformers import AutoProcessor
from transformers import HunYuanVLForConditionalGeneration
from PIL import Image
import torch

model_name_or_path = "tencent/HunyuanOCR"
processor = AutoProcessor.from_pretrained(model_name_or_path, use_fast=False)
img_path = "path/to/your/image.jpg"
image_inputs = Image.open(img_path)
messages1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": img_path},
            {"type": "text", "text": (
                "Extract all information from the main body of the document image "
                "and represent it in markdown format, ignoring headers and footers. "
                "Tables should be expressed in HTML format, formulas in the document "
                "should be represented using LaTeX format, and the parsing should be "
                "organized according to the reading order."
            )},
        ],
    }
]
messages = [messages1]
texts = [
    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
    for msg in messages
]
inputs = processor(
    text=texts,
    images=image_inputs,
    padding=True,
    return_tensors="pt",
)
model = HunYuanVLForConditionalGeneration.from_pretrained(
    model_name_or_path,
    attn_implementation="eager",
    dtype=torch.bfloat16,
    device_map="auto"
)
with torch.no_grad():
    device = next(model.parameters()).device
    inputs = inputs.to(device)
    generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=False)
if "input_ids" in inputs:
    input_ids = inputs.input_ids
else:
    print("inputs: # fallback", inputs)
    input_ids = inputs.inputs
generated_ids_trimmed = [
    out_ids[len(in_ids):] for in_ids, out_ids in zip(input_ids, generated_ids)
]
output_texts = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_texts)
```

#### æˆ–è€…ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨æä¾›çš„æ¨ç†è„šæœ¬ï¼š
```shell
cd Hunyuan-OCR-master/Hunyuan-OCR-hf && python run_hy_ocr.py
```

## ğŸ’¬ æ¨èçš„OCRä»»åŠ¡æç¤ºè¯
| ä»»åŠ¡ | ä¸­æ–‡æç¤ºè¯ | è‹±æ–‡æç¤ºè¯ |
|------|---------|---------|
| **æ–‡å­—æ£€æµ‹è¯†åˆ«** | æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚ | Detect and recognize text in the image, and output the text coordinates in a formatted manner. |
| **æ–‡æ¡£è§£æ** | â€¢ è¯†åˆ«å›¾ç‰‡ä¸­çš„å…¬å¼ï¼Œç”¨ LaTeX æ ¼å¼è¡¨ç¤ºã€‚<br><br>â€¢ æŠŠå›¾ä¸­çš„è¡¨æ ¼è§£æä¸º HTMLã€‚<br><br>â€¢ è§£æå›¾ä¸­çš„å›¾è¡¨ï¼Œå¯¹äºæµç¨‹å›¾ä½¿ç”¨ Mermaid æ ¼å¼è¡¨ç¤ºï¼Œå…¶ä»–å›¾è¡¨ä½¿ç”¨ Markdown æ ¼å¼è¡¨ç¤ºã€‚<br><br>â€¢ æå–æ–‡æ¡£å›¾ç‰‡ä¸­æ­£æ–‡çš„æ‰€æœ‰ä¿¡æ¯ç”¨ markdown æ ¼å¼è¡¨ç¤ºï¼Œå…¶ä¸­é¡µçœ‰ã€é¡µè„šéƒ¨åˆ†å¿½ç•¥ï¼Œè¡¨æ ¼ç”¨ html æ ¼å¼è¡¨è¾¾ï¼Œæ–‡æ¡£ä¸­å…¬å¼ç”¨ latex æ ¼å¼è¡¨ç¤ºï¼ŒæŒ‰ç…§é˜…è¯»é¡ºåºç»„ç»‡è¿›è¡Œè§£æã€‚ | â€¢ Identify the formula in the image and represent it using LaTeX format.<br><br>â€¢ Parse the table in the image into HTML.<br><br>â€¢ Parse the chart in the image; use Mermaid format for flowcharts and Markdown for other charts.<br><br>â€¢ Extract all information from the main body of the document image and represent it in markdown format, ignoring headers and footers. Tables should be expressed in HTML format, formulas in the document should be represented using LaTeX format, and the parsing should be organized according to the reading order. |
| **ä¿¡æ¯æŠ½å–** | â€¢ è¾“å‡º Key çš„å€¼ã€‚<br><br>â€¢ æå–å›¾ç‰‡ä¸­çš„: ['key1','key2', ...] çš„å­—æ®µå†…å®¹ï¼Œå¹¶æŒ‰ç…§ JSON æ ¼å¼è¿”å›ã€‚<br><br>â€¢ æå–å›¾ç‰‡ä¸­çš„å­—å¹•ã€‚ | â€¢ Output the value of Key.<br><br>â€¢ Extract the content of the fields: ['key1','key2', ...] from the image and return it in JSON format.<br><br>â€¢ Extract the subtitles from the image. |
| **ç¿»è¯‘** | å…ˆæå–æ–‡å­—ï¼Œå†å°†æ–‡å­—å†…å®¹ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚è‹¥æ˜¯æ–‡æ¡£ï¼Œåˆ™å…¶ä¸­é¡µçœ‰ã€é¡µè„šå¿½ç•¥ã€‚å…¬å¼ç”¨latexæ ¼å¼è¡¨ç¤ºï¼Œè¡¨æ ¼ç”¨htmlæ ¼å¼è¡¨ç¤ºã€‚ | First extract the text, then translate the text content into English. If it is a document, ignore the header and footer. Formulas should be represented in LaTeX format, and tables should be represented in HTML format. |

## ğŸ“Š è¯„æµ‹æŒ‡æ ‡

> **Note**: ç«å“è¯„æµ‹æŒ‡æ ‡æ¥æºï¼šå®˜æ–¹reportæŒ‡æ ‡(å¦‚æœ‰)ï¼Œæˆ–ç«å“æ¨¡å‹/æ¥å£ä»¥åŠæ¨èçš„æ ‡å‡†æŒ‡ä»¤å¤ç°æ‰€å¾—(å¦‚æ— )ã€‚

> **Note**: HunyuanOCRè¯„æµ‹æŒ‡æ ‡ä½¿ç”¨TensorRTæ¡†æ¶æ¨ç†æ‰€å¾—ï¼Œå¯èƒ½ä¸Transformers/vLLMçš„æ¨ç†æ–¹å¼å­˜åœ¨è½»å¾®å·®å¼‚ã€‚

### è‡ªå»ºè¯„æµ‹é›†ä¸Šçš„æ–‡å­—æ£€æµ‹è¯†åˆ«æŒ‡æ ‡

| Model Type | Methods | Overall | Art | Doc | Game | Hand | Ads | Receipt | Screen | Scene | Video |
|------------|---------|---------|-----|-----|------|------|-----|----------|---------|--------|--------|
| **Traditional methods** | PaddleOCR | 53.38 | 32.83 | 70.23 | 51.59 | 56.39 | 57.38 | 50.59 | 63.38 | 44.68 | 53.35 |
| **Traditional methods** | BaiduOCR | 61.9 | 38.5 | **78.95** | 59.24 | 59.06 | 66.7 | **63.66** | 68.18 | 55.53 | 67.38 |
| **General VLM** | Qwen3VL-2B-Instruct | 29.68 | 29.43 | 19.37 | 20.85 | 50.57 | 35.14 | 24.42 | 12.13 | 34.90 | 40.1 |
| **General VLM** | Qwen3VL-235B-Instruct | 53.62 | 46.15 | 43.78 | 48.00 | 68.90 | 64.01 | 47.53 | 45.91 | 54.56 | 63.79 |
| **General VLM** | Seed-1.6-Vision | 59.23 | 45.36 | 55.04 | 59.68 | 67.46 | 65.99 | 55.68 | 59.85 | 53.66 | 70.33 |
| **OCR-Specific VLM** | HunyuanOCR | **70.92** | **56.76** | 73.63 | **73.54** | **77.10** | **75.34** | 63.51 | **76.58** | **64.56** | **77.31** |

> **æ€»ç»“**: HunyuanOCR OCRåœ¨å„ç§åœºæ™¯ä¸‹å‡å–å¾—äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ï¼ˆ70.92%ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„OCRæ–¹æ³•å’Œå¸¸è§çš„VLMã€‚

### OmniDocBench ä»¥åŠè‡ªå»ºå¤šè¯­ç§è¯„æµ‹é›†ä¸Šçš„æ–‡æ¡£è§£ææŒ‡æ ‡ (ä½¿ç”¨ç¼–è¾‘è·ç¦»è¯„æµ‹)

| Model Type | Method | Size | OmniDocBench | | | | Wild-OmniDocBench | | | | DocML |
|:-----------|:-------|:-----|:---------|:---------|:----------|:--------|:----------|:---------|:----------|:---------|:--------|
| | | | overall | text | formula | table | overall | text | formula | table | |
| **General VLMs** | Gemni-2.5-pro | - | 88.03 | 0.075 | 85.92 | 85.71 | - | - | - | - | 82.64 |
| **General VLMs** | Qwen3-VL-235B | 235B | 89.15 | 0.069 | 88.14 | 86.21 | 79.69 | 0.09 | 80.67 | 68.31 | 81.40 |
| **Specialized VLMs (Modular)** | MonkeyOCR-pro-3B | 3B | 88.85 | 0.075 | 87.5 | 86.78 | 70.00 | 0.211 | 63.27 | 67.83 | 56.50 |
| **Specialized VLMs (Modular)** | MinerU2.5 | 1.2B | 90.67 | 0.047 | 88.46 | 88.22 | 70.91 | 0.218 | 64.37 | 70.15 | 52.05 |
| **Specialized VLMs (Modular)** | PaddleOCR-VL | 0.9B | 91.93 | 0.039 | 88.67 | 91.01 | 72.19 | 0.232 | 65.54 | 74.24 | 57.42 |
| **Specialized VLMs (End2End)** | Mistral-OCR | - | 78.83 | 0.164 | 82.84 | 70.03 | - | - | - | - | 64.71 |
| **Specialized VLMs (End2End)** | Deepseek-OCR | 3B | 87.01 | 0.073 | 83.37 | 84.97 | 74.23 | 0.178 | 70.07 | 70.41 | 57.22 |
| **Specialized VLMs (End2End)** | dots.ocr | 3B | 88.41 | 0.048 | 83.22 | 86.78 | 78.01 | 0.121 | 74.23 | 71.89 | 77.50 |
| **Specialized VLMs (End2End)** | **HunyuanOCR** | 1B | **94.10** | 0.042 | **94.73** | **91.81** | **85.21** | **0.081** | **82.09** | **81.64** | **91.03** |

> **æ€»ç»“**: HunyuanOCR åœ¨å¤šè¯­ç§æ–‡æ¡£è§£ææ–¹é¢å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œåœ¨å¤§å¤šæ•°ç±»åˆ«ä¸­å®ç°äº†æœ€ä½çš„ç¼–è¾‘è·ç¦»ã€‚

### ä¿¡æ¯æŠ½å– (è‡ªå»ºè¯„æµ‹é›†) å’Œ OCRbenchçš„æŒ‡æ ‡

| Model | Cards | Receipts | Video Subtitles | OCRBench |
|:------|:------|:---------|:----------------|:----------|
| DeepSeek-OCR | 10.04 | 40.54 | 5.41 | 430 |
| PP-ChatOCR | 57.02 | 50.26 | 3.1 | - |
| Qwen3-VL-2B-Instruct | 67.62 | 64.62 | 3.75 | 858 |
| Seed-1.6-Vision | 70.12 | 67.5 | 60.45 | 881 |
| Qwen3-VL-235B-A22B-Instruct | 75.59 | 78.4 | 50.74 | **920** |
| Gemini-2.5-Pro | 80.59 | 80.66 | 53.65 | 872 |
| **HunyuanOCR** | **92.29** | **92.53** | **92.87** | 860 |

> **æ€»ç»“**: HunyuanOCR åœ¨å¡è¯ç¥¨æ®ä¿¡æ¯æŠ½å–å’Œè§†é¢‘å­—å¹•æå–ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½å‡æ˜¾è‘—ä¼˜äºå¸¸è§çš„VLMæ¨¡å‹ï¼ŒåŒæ—¶åœ¨OCRBenchä¸Šä¹Ÿè¾¾åˆ°äº†åŒæ ·é‡çº§æ¨¡å‹çš„SOTAæ•ˆæœã€‚

### è‡ªå»ºè¯„æµ‹é›†ä¸Šçš„æ‹ç…§ç¿»è¯‘æŒ‡æ ‡

| Method | Size | Other2En | Other2Zh | DoTA (en2zh) |
|--------|------|-----------|-----------|--------------|
| Gemini-2.5-Flash | - | 79.26 | 80.06 | 85.60 |
| Qwen3-VL-235B-Instruct | 235B | 73.67 | 77.20 | 80.01 |
| Qwen3-VL-8B-Instruct | 4B | 75.09 | 75.63 | 79.86 |
| Qwen3-VL-4B-Instruct | 4B | 70.38 | 70.29 | 78.45 |
| Qwen3-VL-2B-Instruct | 2B | 66.30 | 66.77 | 73.49 |
| **HunyuanOCR** | **1B** | 73.38 | 73.62 | 83.48 |

> **æ€»ç»“**: HunyuanOCRä»…ä½¿ç”¨1Bå‚æ•°é‡ï¼Œåœ¨æ‹ç…§ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†ä¸Gemini-2.5-Flashä»¥åŠQwen3-VL-235Bç›¸å½“çš„æ•ˆæœã€‚

## ğŸ’¡ æ•ˆæœå¯è§†åŒ–
<details>
<summary><u style="color: #2E64FE;">ç‚¹å‡»å±•å¼€è¯¦æƒ…</u></summary>

### æ–‡å­—æ£€æµ‹è¯†åˆ«

æ—¨åœ¨å¯¹æ–‡å­—å›¾åƒä¸­å‡ºç°çš„æ‰€æœ‰æ–‡å­—ï¼ŒæŒ‰ç…§è¡Œçº§åˆ«ç²’åº¦è¿›è¡Œæ–‡æœ¬å†…å®¹å’Œå¯¹åº”çš„åæ ‡ä¿¡æ¯è¾“å‡ºã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯¹æ–‡æ¡£ã€è‰ºæœ¯å­—ã€è¡—æ™¯ã€æ‰‹å†™ã€å¹¿å‘Šã€ç¥¨æ®ã€æˆªå±ã€æ¸¸æˆã€è§†é¢‘ç­‰åœºæ™¯ä¸Šè¡¨ç°å“è¶Šã€‚


<p align="left">
 <img src="./assets/spotting1_cropped.png" width="40%"/> <br>
 <img src="./assets/vis_document_23.jpg" width="40%"/> <br>
</p>


### å¤æ‚æ–‡æ¡£è§£æ

å¯¹å¤šè¯­ç§æ–‡æ¡£æ‰«æä»¶æˆ–æ‹æ‘„å›¾åƒè¿›è¡Œç”µå­åŒ–ï¼Œå…·ä½“åœ°ï¼Œæ˜¯å°†å›¾ç‰‡ä¸­å‡ºç°çš„æ–‡æœ¬å†…å®¹æŒ‰ç…§é˜…è¯»é¡ºåºè¿›è¡Œç»„ç»‡ã€å…¬å¼é‡‡ç”¨Latexæ ¼å¼ã€å¤æ‚è¡¨æ ¼é‡‡ç”¨HTMLæ ¼å¼è¡¨è¾¾ã€‚

<p align="left">
 <img src="./assets/vis_parsing_fig.png" width="40%"/> <br>
  <img src="./assets/show_res_parsing_fig.png" width="40%"/> <br>
  <img src="./assets/vis_parsing_table.png" width="40%"/> <br>
  <img src="./assets/vis_parsing_table_2.png" width="40%"/> <br>
  <img src="./assets/parsing_rgsj.png" width="40%"/> <br>
  <img src="./assets/parsing_rgsjz_2.png" width="40%"/> <br>
  <img src="./assets/qikai1.png" width="40%"/> <br>
  <img src="./assets/guwan1.png" width="40%"/> <br>
  <img src="./assets/parsing_chart1.png" width="40%"/> <br>
  <img src="./assets/vis_parsing_chart1.png" width="40%"/> <br>
  <img src="./assets/vis_parsing_chart2.png" width="40%"/> <br>
  <img src="./assets/vis_parsing_chart3.png" width="40%"/> <br>
</p>


### å¼€æ”¾å­—æ®µä¿¡æ¯æå–

å¯¹å¸¸è§å¡è¯å’Œç¥¨æ®çš„æ„Ÿå…´è¶£å­—æ®µï¼ˆå¦‚å§“å/åœ°å€/å•ä½ç­‰ï¼‰ï¼Œé‡‡ç”¨æ ‡å‡†çš„jsonæ ¼å¼è§£æã€‚

<p align="left">
 <img src="./assets/vis_ie_1.png" width="40%"/> <br>
</p>

<p align="left">
 <img src="./assets/ie_parallel.jpg" width="25%"/> <br>
</p>

**Prompt:**
æå–å›¾ç‰‡ä¸­çš„ï¼š['å•ä»·', 'ä¸Šè½¦æ—¶é—´', 'å‘ç¥¨å·ç ', 'çœå‰ç¼€', 'æ€»é‡‘é¢', 'å‘ç¥¨ä»£ç ', 'ä¸‹è½¦æ—¶é—´', 'é‡Œç¨‹æ•°']çš„å­—æ®µå†…å®¹ï¼Œå¹¶ä¸”æŒ‰ç…§JSONæ ¼å¼è¿”å›ã€‚

**Response:**
```json
{
    "å•ä»·": "3.00",
    "ä¸Šè½¦æ—¶é—´": "09:01",
    "å‘ç¥¨å·ç ": "42609332",
    "çœå‰ç¼€": "é™•",
    "æ€»é‡‘é¢": "ï¿¥77.10å…ƒ",
    "å‘ç¥¨ä»£ç ": "161002018100",
    "ä¸‹è½¦æ—¶é—´": "09:51",
    "é‡Œç¨‹æ•°": "26.1km"
}
```

### è§†é¢‘å­—å¹•æå–

èƒ½å¤Ÿå¯¹è§†é¢‘çš„å­—å¹•å®ç°è‡ªåŠ¨åŒ–æŠ½å–ï¼ŒåŒ…æ‹¬åŒè¯­å­—å¹•ã€‚


<p align="left">
 <img src="./assets/vis_subtitle1.png" width="40%"/> <br>
 <img src="./assets/vis_subtitle2.png" width="40%"/> <br>
 <img src="./assets/vis_subtitle3.png" width="37.5%"/> <br>
</p>




### å›¾ç‰‡ç¿»è¯‘åŠŸèƒ½

å¯¹æ‹ç…§æˆ–è€…å­—å…¸æ–‡æ¡£çš„å¤šè¯­ç§å›¾ç‰‡èƒ½å¤Ÿè¿›è¡Œç«¯åˆ°ç«¯ç¿»è¯‘æˆä¸­æ–‡æˆ–è‹±æ–‡çš„æ–‡æœ¬æ ¼å¼è¾“å‡ºï¼Œç›®å‰ä¸»è¦æ”¯æŒ14ç§é«˜é¢‘åº”ç”¨å°è¯­ç§ï¼ˆå…·ä½“åŒ…æ‹¬ï¼šå¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€åœŸè€³å…¶è¯­ã€æ„å¤§åˆ©è¯­ã€ä¿„è¯­ã€æ³•è¯­ã€è‘¡è„ç‰™è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€æ³°è¯­ã€è¶Šå—è¯­ã€å°å°¼è¯­ã€é©¬æ¥è¯­ã€æ—¥è¯­ã€éŸ©è¯­ï¼‰ç¿»è¯‘æˆä¸­/è‹±æ–‡ï¼Œä»¥åŠä¸­è‹±äº’è¯‘åŠŸèƒ½ï¼ˆå‚åŠ ICDAR2025æ–‡æ¡£ç«¯åˆ°ç«¯ç¿»è¯‘æ¯”èµ›èµ¢å¾—äº†å°æ¨¡å‹èµ›é“å† å†›æˆç»©ï¼‰ã€‚


<p align="left">
 <img src="./assets/translation2.png" width="40%"/> <br>
</p>

</details>


## ğŸ“š å¼•ç”¨
@misc{hunyuanocr2025,
    title={HunyuanOCR Technical Report},
    author={Tencent Hunyuan Vision Team},
    year={2025},
    publisher={GitHub},
    journal={GitHub repository},
    howpublished={\url{https://github.com/Tencent-Hunyuan/HunyuanOCR}}
}

## ğŸ™ è‡´è°¢
æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢[PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)ã€[MinerU](https://github.com/opendatalab/MinerU)ã€[MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR)ã€[DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR)ã€[dots.ocr](https://github.com/rednote-hilab/dots.ocr) çš„ä½œè€…å’Œè´¡çŒ®è€…ï¼Œæ„Ÿè°¢ä»–ä»¬æ°å‡ºçš„å¼€æºå·¥ä½œå’Œå®è´µçš„ç ”ç©¶æ€è·¯ã€‚

åŒæ—¶æˆ‘ä»¬ä¹Ÿæ„Ÿè°¢ä»¥ä¸‹å®è´µçš„å¼€æºæ•°æ®é›†ï¼š[OminiDocBench](https://github.com/opendatalab/OmniDocBench)ã€[OCRBench](https://github.com/Yuliang-Liu/MultimodalOCR/tree/main/OCRBench)ã€[DoTA](https://github.com/liangyupu/DIMTDA)ã€‚

ç‰¹åˆ«æ„Ÿè°¢vLLMç¤¾åŒºåœ¨æ¨ç†éƒ¨ç½²æ–¹é¢æ‰€æä¾›çš„å³æ—¶æ”¯æŒã€‚
