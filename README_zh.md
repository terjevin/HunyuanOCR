<p align="center">
 <img src="./assets/hunyuan_logo.png" width="400"/> <br>
</p>

<div align="center">

[ä¸­æ–‡](./README_zh.md) | [English](./README.md)

# HunyuanOCR: åŸºäºæ··å…ƒæ¶æ„çš„1Bå‚æ•°ç«¯åˆ°ç«¯å¤šè¯­è¨€OCRæ¨¡å‹

æ··å…ƒåŸç”Ÿå¤šæ¨¡æ€ç«¯åˆ°ç«¯ OCR ä¸“å®¶ï¼Œ1B è½»é‡åŒ–å‚æ•°å´æ–©è·å¤šé¡¹ä¸šç•Œ SOTAï¼
ç²¾é€šå¤æ‚æ–‡æ¡£è§£æï¼Œå…¼å…·å¤šè¯­ç§æ–‡å­—è¯†åˆ«ã€ç¥¨æ®å­—æ®µæŠ½å–ã€å­—å¹•æå–ã€æ‹ç…§ç¿»è¯‘ç­‰å…¨åœºæ™¯å®ç”¨æŠ€èƒ½ï¼

ğŸ“‘ è®ºæ–‡ä¸æ¨¡å‹æƒé‡å³å°†å‘å¸ƒï¼

[æ¼”ç¤º](#æ¼”ç¤º) | [å®‰è£…](#å®‰è£…) | [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) | [æ–‡æ¡£](#æ–‡æ¡£)

</div>

## ğŸ”¥ æœ€æ–°åŠ¨æ€
- **[2025/11/20]** ğŸ“ æ¨ç†ä»£ç å’Œæ¨¡å‹æƒé‡å·²å¼€æºã€‚

## ğŸ“– ç®€ä»‹
HunyuanOCRæ˜¯ä¸€æ¬¾åŸºäºæ··å…ƒåŸç”Ÿå¤šæ¨¡æ€æ¶æ„çš„ç«¯åˆ°ç«¯OCRä¸“å®¶æ¨¡å‹ã€‚ä»…ä»¥1Bè½»é‡åŒ–å‚æ•°ï¼Œä¾¿å·²æ–©è·å¤šé¡¹ä¸šç•ŒSOTAæˆç»©ã€‚è¯¥æ¨¡å‹ç²¾é€šå¤æ‚æ–‡æ¡£è§£æï¼ŒåŒæ—¶åœ¨å¤šè¯­ç§æ–‡å­—è¯†åˆ«ã€ç¥¨æ®å­—æ®µæŠ½å–ã€å­—å¹•æå–ã€æ‹ç…§ç¿»è¯‘ç­‰å…¨åœºæ™¯å®ç”¨æŠ€èƒ½ä¸­è¡¨ç°å‡ºè‰²ã€‚

åŸºäºè…¾è®¯æ··å…ƒæŠ€æœ¯æ‰“é€ ï¼Œè¯¥æ¨¡å‹é€šè¿‡ç«¯åˆ°ç«¯æ¶æ„è®¾è®¡å’Œå•æ¬¡æ¨ç†ï¼Œæä¾›å“è¶Šæ€§èƒ½çš„åŒæ—¶å¤§å¹…ç®€åŒ–éƒ¨ç½²æµç¨‹ï¼Œåœ¨ä¸ä¼ ç»Ÿçº§è”ç³»ç»Ÿå’Œå•†ç”¨APIçš„å¯¹æ¯”ä¸­ä¿æŒç«äº‰ä¼˜åŠ¿ã€‚

<div align="center">
  <img src="./assets/hyocr-pipeline.png" alt="HunyuanOCRæ¡†æ¶" width="80%">
</div>

## âœ¨ æ ¸å¿ƒç‰¹ç‚¹

- ğŸ’ª **è½»é‡åŒ–æ¶æ„**ï¼šåŸºäºæ··å…ƒåŸç”Ÿå¤šæ¨¡æ€æ¶æ„ä¸è®­ç»ƒç­–ç•¥ï¼Œæ‰“é€ ä»…1Bå‚æ•°çš„OCRä¸“é¡¹æ¨¡å‹ï¼Œå¤§å¹…é™ä½éƒ¨ç½²æˆæœ¬ã€‚

- ğŸ“‘ **å…¨åœºæ™¯åŠŸèƒ½**ï¼šå•ä¸€æ¨¡å‹è¦†ç›–æ–‡å­—æ£€æµ‹å’Œè¯†åˆ«ã€å¤æ‚æ–‡æ¡£è§£æã€ç¥¨æ®å­—æ®µæŠ½å–ã€å­—å¹•æå–ç­‰OCRç»å…¸ä»»åŠ¡ï¼Œæ›´æ”¯æŒç«¯åˆ°ç«¯æ‹ç…§ç¿»è¯‘ä¸æ–‡æ¡£é—®ç­”ã€‚

- ğŸš€ **æè‡´æ˜“ç”¨**ï¼šæ·±åº¦è´¯å½»å¤§æ¨¡å‹"ç«¯åˆ°ç«¯"ç†å¿µï¼Œå•ä¸€æŒ‡ä»¤ã€å•æ¬¡æ¨ç†ç›´è¾¾SOTAç»“æœï¼Œè¾ƒä¸šç•Œçº§è”æ–¹æ¡ˆæ›´é«˜æ•ˆä¾¿æ·ã€‚

- ğŸŒ **å¤šè¯­ç§æ”¯æŒ**ï¼šæ”¯æŒè¶…è¿‡100ç§è¯­è¨€ï¼Œåœ¨å•è¯­ç§å’Œæ··åˆè¯­è¨€åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“‹ æ¨¡å‹å¡ç‰‡

| ç»„ä»¶ | æ¶æ„ | å‚æ•°é‡ | åŠŸèƒ½ |
|-----------|-------------|------------|-----------|
| è§†è§‰ç¼–ç å™¨ | SigVLIP-v2 (ViT-based) | 400M | å›¾åƒå¤„ç†ä¸ç‰¹å¾æå– |
| è¯­è¨€æ¨¡å‹ | Hunyuan-LLM | 500M | æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆ |
| è§†è§‰è¯­è¨€æ¡¥æ¥ | MLP Adapter | 90M | å¤šæ¨¡æ€ç‰¹å¾èåˆ |
| **æ€»è®¡** | - | **~1B** | ç«¯åˆ°ç«¯OCRä¸æ–‡æ¡£ç†è§£ |

## ğŸ› ï¸ ç¯å¢ƒä¾èµ–ä¸å®‰è£…

### ç³»ç»Ÿè¦æ±‚
- ğŸ–¥ï¸ æ“ä½œç³»ç»Ÿï¼šLinux
- ğŸ Pythonç‰ˆæœ¬ï¼š3.12+ï¼ˆæ¨èï¼‰
- âš¡ CUDAç‰ˆæœ¬ï¼š12.8
- ğŸ”¥ PyTorchç‰ˆæœ¬ï¼š2.7.1
- ğŸ® GPUï¼šæ”¯æŒCUDAçš„NVIDIAæ˜¾å¡
- ğŸ§  GPUæ˜¾å­˜ï¼šâ‰¥3Ã—80GBï¼ˆæ¨è4Ã—80GBä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰
- ğŸ’¾ ç£ç›˜ç©ºé—´ï¼š170GBï¼ˆç”¨äºå­˜å‚¨æ¨¡å‹æƒé‡ï¼‰

### å®‰è£…æ­¥éª¤
```bash
pip install https://mirrors.tencent.com/repository/generic/transformers/transformers-4.56.0.post2-py3-none-any.whl
pip install opencv-python-headless
pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu128
```

## ğŸš€ åŸºäºTransformerså¿«é€Ÿä½¿ç”¨

### æ¨¡å‹æ¨ç†

HunyuanOCR æä¾›ç›´è§‚çš„æ¨¡å‹æ¨ç†æ¥å£ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨æŒ‡å¼•ï¼š

#### 1. å¯¼å…¥ä¾èµ–åº“
```python
import os
import torch
import numpy as np
from PIL import Image
from transformers import AutoProcessor, HunYuanVLV1ForConditionalGeneration
from qwen_vl_utils import process_vision_info
```

#### 2. åŠ è½½æ¨¡å‹
```python
def load_model():
    # Set GPU device
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    
    # Load processor and model
    processor = AutoProcessor.from_pretrained(
        "PATH_TO_MODEL",
        use_fast=False,
        trust_remote_code=True
    )
    
    model = HunYuanVLV1ForConditionalGeneration.from_pretrained(
        "PATH_TO_MODEL",
        attn_implementation="eager",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    return model, processor
```

#### 3. æ¨ç†ä»£ç 
```python
def inference(text: str, img_path: str, model, processor) -> list:
    # Construct input format
    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": img_path},
            {"type": "text", "text": text},
        ],
    }]
    
    # Process inputs
    texts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
             for msg in messages]
    image_inputs, video_inputs = process_vision_info(messages)
    
    # Model inference
    inputs = processor(
        text=texts,
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    ).to("cuda")
    
    # Generate results
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=1024*8,
            repetition_penalty=1.03,
            do_sample=False
        )
    
    return processor.batch_decode(
        output[:, inputs.input_ids.shape[1]:],
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )
```

#### 4. ä½¿ç”¨ç¤ºä¾‹
```python
# Load model
model, processor = load_model()

# Single image inference
img_path = "path/to/your/image.jpg"
query = "Please extract the text content from this image"
result = inference(text=query, img_path=img_path, model=model, processor=processor)
print("Inference result:", result)

# Batch processing
import json
from tqdm import tqdm

with open("input.jsonl", "r") as fin, open("output.jsonl", "w") as fout:
    for line in tqdm(fin):
        data = json.loads(line)
        result = inference(
            text=data["question"],
            img_path=data["img_path"],
            model=model,
            processor=processor
        )
        data["response"] = result
        fout.write(json.dumps(data, ensure_ascii=False) + "\n")
```

## ğŸ’¬ æ¨èçš„OCRä»»åŠ¡æç¤ºè¯
| ä»»åŠ¡ | ä¸­æ–‡æç¤ºè¯ | è‹±æ–‡æç¤ºè¯ |
|------|---------|---------|
| **æ–‡å­—æ£€æµ‹è¯†åˆ«** | æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚ | Detect and recognize text in the image, and output the text coordinates in a formatted manner. |
| **æ–‡æ¡£è§£æ** | â€¢ è¯†åˆ«å›¾ç‰‡ä¸­çš„å…¬å¼ï¼Œç”¨ LaTeX æ ¼å¼è¡¨ç¤ºã€‚<br><br>â€¢ æŠŠå›¾ä¸­çš„è¡¨æ ¼è§£æä¸º HTMLã€‚<br><br>â€¢ è§£æå›¾ä¸­çš„å›¾è¡¨ï¼Œå¯¹äºæµç¨‹å›¾ä½¿ç”¨ Mermaid æ ¼å¼è¡¨ç¤ºï¼Œå…¶ä»–å›¾è¡¨ä½¿ç”¨ Markdown æ ¼å¼è¡¨ç¤ºã€‚<br><br>â€¢ æå–æ–‡æ¡£å›¾ç‰‡ä¸­æ­£æ–‡çš„æ‰€æœ‰ä¿¡æ¯ç”¨ markdown æ ¼å¼è¡¨ç¤ºï¼Œå…¶ä¸­é¡µçœ‰ã€é¡µè„šéƒ¨åˆ†å¿½ç•¥ï¼Œè¡¨æ ¼ç”¨ html æ ¼å¼è¡¨è¾¾ï¼Œæ–‡æ¡£ä¸­å…¬å¼ç”¨ latex æ ¼å¼è¡¨ç¤ºï¼ŒæŒ‰ç…§é˜…è¯»é¡ºåºç»„ç»‡è¿›è¡Œè§£æã€‚ | â€¢ Identify the formula in the image and represent it using LaTeX format.<br><br>â€¢ Parse the table in the image into HTML.<br><br>â€¢ Parse the chart in the image; use Mermaid format for flowcharts and Markdown for other charts.<br><br>â€¢ Extract all information from the main body of the document image and represent it in markdown format, ignoring headers and footers. Tables should be expressed in HTML format, formulas in the document should be represented using LaTeX format, and the parsing should be organized according to the reading order. |
| **ä¿¡æ¯æŠ½å–** | â€¢ è¯·è¾“å‡º Key çš„å€¼ã€‚<br><br>â€¢ æå–å›¾ç‰‡ä¸­çš„: ['key1','key2', ...] çš„å­—æ®µå†…å®¹ï¼Œå¹¶æŒ‰ç…§ JSON æ ¼å¼è¿”å›ã€‚<br><br>â€¢ æå–å›¾ç‰‡ä¸­çš„å­—å¹•ã€‚ | â€¢ Please output the value of Key.<br><br>â€¢ Extract the content of the fields: ['key1','key2', ...] from the image and return it in JSON format.<br><br>â€¢ Extract the subtitles from the image. |
| **ç¿»è¯‘** | å…ˆæå–æ–‡å­—ï¼Œå†å°†æ–‡å­—å†…å®¹ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚è‹¥æ˜¯æ–‡æ¡£ï¼Œåˆ™å…¶ä¸­é¡µçœ‰ã€é¡µè„šå¿½ç•¥ã€‚å…¬å¼ç”¨latexæ ¼å¼è¡¨ç¤ºï¼Œè¡¨æ ¼ç”¨htmlæ ¼å¼è¡¨ç¤ºã€‚ | First extract the text, then translate the text content into English. If it is a document, ignore the header and footer. Formulas should be represented in LaTeX format, and tables should be represented in HTML format. |

## ğŸ“Š è¯„æµ‹æŒ‡æ ‡

### è‡ªå»ºè¯„æµ‹é›†ä¸Šçš„æ–‡å­—æ£€æµ‹è¯†åˆ«æ•ˆæœ

| Model Type | Methods | Overall | Art | Doc | Game | Hand | Ads | Receipt | Screen | Scene | Video |
|------------|---------|---------|-----|-----|------|------|-----|----------|---------|--------|--------|
| **Traditional methods** | PaddleOCR | 53.38 | 32.83 | 70.23 | 51.59 | 56.39 | 57.38 | 50.59 | 63.38 | 44.68 | 53.35 |
| | BaiduOCR | 61.9 | 38.5 | **78.95** | 59.24 | 59.06 | 66.7 | **63.66** | 68.18 | 55.53 | 67.38 |
| **General VLM** | Qwen3VL-2B-Instruct | 29.68 | 29.43 | 19.37 | 20.85 | 50.57 | 35.14 | 24.42 | 12.13 | 34.90 | 40.1 |
| | Qwen3VL-235B-Instruct | 53.62 | 46.15 | 43.78 | 48.00 | 68.90 | 64.01 | 47.53 | 45.91 | 54.56 | 63.79 |
| | Seed1.6-VL-Instruct | 59.23 | 45.36 | 55.04 | 59.68 | 67.46 | 65.99 | 55.68 | 59.85 | 53.66 | 70.33 |
| **OCR-Specific VLM** | HunyuanOCR | **70.92** | **56.76** | 73.63 | **73.54** | **77.10** | **75.34** | 63.51 | **76.58** | **64.56** | **77.31** |

> **æ€»ç»“**: HunyuanOCR OCRåœ¨å„ç§åœºæ™¯ä¸‹å‡å–å¾—äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ï¼ˆ70.92%ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„OCRæ–¹æ³•å’Œå¸¸è§çš„VLMã€‚

### OmniDocBench ä¸Šçš„æ–‡æ¡£è§£ææ•ˆæœ (ä½¿ç”¨ç¼–è¾‘è·ç¦»è¯„æµ‹)

| Model | English |  |  |  | Chinese |  |  |  |
|-------|---------|---------|----------|--------|----------|---------|----------|---------|
|  | overall | text | formula | table | overall | text | formula | table |
| Dolphin | 0.356 | 0.352 | 0.465 | 0.258 | 0.44 | 0.44 | 0.604 | 0.367 |
| SmolDocling | 0.493 | 0.262 | 0.753 | 0.729 | 0.816 | 0.838 | 0.997 | 0.907 |
| dots.ocr-3B | 0.182 | 0.137 | 0.320 | 0.166 | 0.261 | 0.229 | 0.468 | 0.160 |
| HunyuanOCR | **0.123** | 0.049 | **0.242** | 0.147 | **0.157** | 0.087 | **0.377** | **0.08** |

> **æ€»ç»“**: HunyuanOCR åœ¨è‹±è¯­å’Œä¸­æ–‡æ–‡æ¡£è§£ææ–¹é¢å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œåœ¨å¤§å¤šæ•°ç±»åˆ«ä¸­å®ç°äº†æœ€ä½çš„ç¼–è¾‘è·ç¦»ã€‚

### ä¿¡æ¯æŠ½å– (è‡ªå»ºè¯„æµ‹é›†) å’Œ OCRbenchçš„æ•ˆæœ

| Model | Cards & Receipts | Video Subtitles | OCRBench |
|-------|------------------|-----------------|-----------|
| DeepSeek-OCR | 25.29 | 5.41 | 430 |
| PP-ChatOCR | 53.64 | 3.1 | - |
| Qwen3VL-2B-Instruct | 66.12 | 3.75 | 858 |
| Seed1.5-VL | 68.81 | 60.45 | 881 |
| Qwen3VL-235B-A22B-Instruct | 77.0 | 50.74 | **920** |
| Gemini-2.5-Pro | 80.63 | 53.65 | 872 |
| **HunyuanOCR (âˆ¼1B)** | **92.41** | **92.87** | 858 |

> **æ€»ç»“**: HunyuanOCR åœ¨å¡è¯ç¥¨æ®ä¿¡æ¯æŠ½å–å’Œè§†é¢‘å­—å¹•æå–ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½å‡æ˜¾è‘—ä¼˜äºå¸¸è§çš„VLMæ¨¡å‹ï¼ŒåŒæ—¶åœ¨OCRBenchä¸Šä¹Ÿè¾¾åˆ°äº†åŒæ ·é‡çº§æ¨¡å‹çš„SOTAæ•ˆæœã€‚

## ğŸ’¡ æ•ˆæœå¯è§†åŒ–
<details>
æ–‡å­—æ£€æµ‹è¯†åˆ«

æ–‡æ¡£è§£æ

ä¿¡æ¯æŠ½å–

ç¿»è¯‘
</details>


## ğŸ“š å¼•ç”¨
@misc{hunyuanocr2025,
    title={HunyuanOCR: Advanced OCR Engine for Document Understanding},
    author={Tencent Hunyuan Team},
    year={2025},
    publisher={GitHub},
    journal={GitHub repository},
    howpublished={\url{https://github.com/Tencent/HunyuanOCR}}
}

## ğŸ™ è‡´è°¢
æ„Ÿè°¢æ‰€æœ‰ä¸ºHunyuanOCRçš„å¼€å‘åšå‡ºè´¡çŒ®çš„äººä»¬

ç‰¹åˆ«æ„Ÿè°¢è…¾è®¯æ··å…ƒå›¢é˜Ÿ

æˆ‘ä»¬æ„Ÿè°¢å¼€æºç¤¾åŒºçš„æ”¯æŒã€‚


## ğŸ“„ è®¸å¯è¯
æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ã€‚
